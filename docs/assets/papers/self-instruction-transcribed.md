# SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions

## Abstract
Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to gener- alize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruc- tion data that is often limited in quantity, di- versity, and creativity, therefore hindering the generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities of pre- trained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or sim- ilar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% abso- lute improvement over the original model on SUPER-NATURALINSTRUCTIONS, on par with the performance of InstructGPT001,1 which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT001. SELF-INSTRUCT provides an almost annotation-free method for aligning pre- trained language models with instructions, and we release our large synthetic dataset to facili- tate future studies on instruction tuning.2

## Introduction

The recent NLP literature has witnessed a tremen- dous amount of activity in building models that 
can follow natural language instructions (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022; Chung et al., 2022, i.a.). These developments are powered by two key components: large pretrained language mod- els (LM) and human-written instruction data (e.g., PROMPTSOURCE (Bach et al., 2022) and SUPER- NATURALINSTRUCTIONS (Wang et al., 2022, SU- PERNI for short)). However, collecting such in- struction data is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a
true variety of tasks and different ways to describe them. Continuing to improve the quality and cov- erage of instruction-tuned models necessitates the development of alternative approaches for supervis- ing the instruction tuning process.

In this work, we introduce SELF-INSTRUCT, a semi-automated process for instruction-tuning a pretrained LM using instructional signals from the model itself. The overall process is an iterative boot- strapping algorithm (see Figure 2), which starts off with a limited (e.g., 175 in our study) seed set of manually-written tasks that are used to guide the overall generation. In the first phase, the model is prompted to generate instructions for new tasks. This step leverages the existing collection of instruc- tions to create more broad-coverage instructions that define (often new) tasks. Given the newly- generated set of instructions, the framework also creates input-output instances for them, which can be later used for supervising the instruction tuning. Finally, various heuristics are used to automatically filter low-quality or repeated instructions, before adding the remaining valid tasks to the task pool. This process can be repeated for many iterations until reaching a large number of tasks.

To evaluate SELF-INSTRUCT empirically, we run this framework on GPT3 (Brown et al., 2020), which is a vanilla LM (§3). The iterative SELF- INSTRUCT process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs. We observe that the resulting

data provides a diverse range of creative tasks, as is demonstrated by examples in Figure 1. These generated tasks deviate from the distribution of typ- ical NLP tasks, and also have fairly small overlap with the seed tasks (§3.2). On this resulting data, we build GPT3SELF-INST by finetuning GPT3 (i.e., the same model used for generating the instruction data). We evaluate GPT3SELF-INST in comparison to various other models on both typical NLP tasks in- cluded in SUPERNI (Wang et al., 2022), and a set of new instructions that are created for novel usage of instruction-following models (§4). The results in- dicate that GPT3SELF-INST outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of InstructGPT001. Moreover, our human evaluation on the newly- created instruction set shows that GPT3SELF-INST demonstrates a broad range of instruction follow- ing ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind InstructGPT001.

In summary, our contributions are: (1) we introduce SELF-INSTRUCT, a method for induc- ing instruction following capabilities with mini- mal human-labeled data; (2) we demonstrate its effectiveness via extensive instruction-tuning ex- periments; and (3) we release a large synthetic dataset of 52K instructions and a set of manually- written novel tasks for building and evaluating fu- ture instruction-following models.

## Method

Annotating large-scale instruction data can be chal- lenging for humans because it requires 1) creativity to come up with novel tasks and 2) expertise for writing the solutions to each task. Here, we de- tail our process for SELF-INSTRUCT, which refers to the pipeline of generating tasks with a vanilla pretrained language model itself, filtering the gen- erated data, and then conducting instruction tuning with this generated data in order to align the LM to follow instructions better. This pipeline is depicted in Figure 2.

### Defining Instruction Data

### Automatic Instruction Data Generation

Our pipeline for data generation consists of four steps: 1) generating task instructions, 2) determin- ing if the instruction represents a classification task, 3) instance generation with either an input-first or output-first approach, and 4) filtering low-quality data.

**Instruction Generation.** At the first step, SELF- INSTRUCT generates new instructions from a small set of seed human-written instructions in a boot- strapping fashion. We initiate the task pool with 175 tasks (1 instruction and 1 instance for each
task).3 For every step, we sample 8 task instruc- tions from this pool as in-context examples. Of the 8 instructions, 6 are from the human-written
tasks, and 2 are from the model-generated tasks in previous steps to promote diversity. The prompting template is shown in Table 5.

Classification Task Identification. Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.4 We prompt the LM in a few-shot way to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks. The prompting template is shown in Table 6.


Instance Generation. Given the instructions and their task type, we generate instances for each in- struction independently. This is challenging be- cause it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and gener- ate them, and finally complete the task by produc- ing the output. We found that pretrained LMs can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks. A natural way to do this is the Input- first Approach, where we can ask an LM to come up with the input fields first based on the instruc- tion, and then produce the corresponding output. This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks. The prompting template is shown in Table 7.


However, we found that this approach can gen- erate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detec- tion, it usually generates grammatical input). There- fore, we additionally propose an Output-first Ap- proach for classification tasks, where we first gener- ate the possible class labels, and then condition the input generation on each class label. The prompting templateisshowninTable8.5 Weapplytheoutput- first approach to the classification tasks identified in the former step, and the input-first approach to the remaining non-classification tasks.

Filtering and Postprocessing. To encourage di- versity, a new instruction is added to the task pool only when its ROUGE-L similarity with any exist-ing instruction is less than 0.7. We also exclude instructions that contain some specific keywords (e.g., image, picture, graph) that usually can not be processed by LMs. When generating new instances for each instruction, we filter out instances that are exactly the same or those with the same input but different outputs. Invalid generations are identified and filtered out based on heuristics (e.g., instruc- tion is too long or too short, instance output is a repetition of the input).

### Finetuning the LM to Follow Instructions

After creating large-scale instruction data, we use it to finetune the original LM (i.e., SELF-INSTRUCT). To do this, we concatenate the instruction and in- stance input as a prompt and train the model to generate the instance output in a standard super- vised way. To make the model robust to different formats, we use multiple templates to encode the instruction and instance input together. For exam- ple, the instruction can be prefixed with “Task:” or not, the input can be prefixed with “Input:” or not,
“Output:” can be appended at the end of the prompt or not, and different numbers of break lines can be put in the middle, etc.

## SELF-INSTRUCT Data from GPT3

In this section, we apply our method for inducing instruction data to GPT3 as a case study. We use the largest GPT3 LM (“davinci” engine) accessed through the OpenAI API.6 The parameters for mak- ing queries are described in Appendix A.2. Here we present an overview of the generated data.

### Statistics

Table 1 describes the basic statistics of the gener- ated data. We generate a total of over 52K instruc- tions and more than 82K instances corresponding to these instructions after filtering.

### Diversity

To study what types of instructions are generated and how diverse they are, we identify the verb-noun structure in the generated instructions. We use the Berkeley Neural Parser7 (Kitaev and Klein, 2018; Kitaev et al., 2019) to parse the instructions and then extract the verb that is closest to the root as well as its first direct noun object. 26,559 out of the 52,445 instructions contain such structure; other instructions usually contain more complex clauses (e.g., “Classify whether this tweet contains political content or not.”) or are framed as questions (e.g., “Which of these statements are true?”). We plot the top 20 most common root verbs and their top 4 direct noun objects in Figure 3, which account for 14% of the entire set. Overall, we see quite diverse intents and textual formats in these instructions.

We further study how the generated instructions differ from the seed instructions used to prompt the generation. For each generated instruction, we compute its highest ROUGE-L overlap with the 175 seed instructions. We plot the distribution of these ROUGE-L scores in Figure 4. The results indicate a decent number of new instructions were generated, which do not have much overlap with the seeds. We also demonstrate diversity in the length of the instructions, instance inputs, and instance outputs in Figure 5.

### Quality

So far, we have shown the quantity and diversity of the generated data, but its quality remains un- certain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (author of this work) to label whether each instance is cor- rect or not, in terms of the instruction, the instance input, and the instance output. Evaluation results in Table 2 show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). How- ever, we found that even though the generations may contain errors, most of them are still in the correct format or partially correct, which can pro- vide useful guidance for training models to follow instructions. We listed a number of good examples and bad examples in Table 10 and 11, respectively.


## Experimental Results

We conduct experiments to measure and compare the performance of models under various instruc- tion tuning setups. We first describe our models and other baselines, followed by our experiments

### GPT3SELF-INST: finetuning GPT3 on its own instruction data

Given the instruction-generated instruction data, we conduct instruction tuning with the GPT3 model itself (“davinci” engine). As described in §2.3, we use various templates to concatenate the instruction
and input, and train the model to generate the output. This finetuning is done through the OpenAI fine- tuning API.8 We use the default hyper-parameters, except that we set the prompt loss weight to 0, and we train the model for 2 epochs. We refer the reader to Appendix A.3 for additional finetuning details. The resulting model is denoted by GPT3 .

### Baselines

**Off-the-shelf LMs**.

We evaluate T5-LM (Lester et al., 2021; Raffel et al., 2020) and GPT3 (Brown et al., 2020) as the vanilla LM baselines (only pre- training, no additional finetuning). These baselines will indicate the extent to which off-the-shelf LMs are capable of following instructions naturally im- mediately after pretraining.

**Publicly available instruction-tuned models.**

T0 and T𝑘-INSTRUCT are two instruction-tuned models proposed in Sanh et al. (2022) and Wang et al. (2022), respectively, and are demonstrated to be able to follow instructions for many NLP tasks. Both of these models are finetuned from the T5 (Raffel et al., 2020) checkpoints and are pub- licly available.9 For both of these models, we use
their largest version with 11B parameters

**Instruction-tuned GPT3 models.**

We evaluate InstructGPT (Ouyang et al., 2022), which is devel- oped by OpenAI based on GPT3 to follow human instructions better and has been found by the com- munity to have impressive zero-shot abilities. There are various generations of these models, where newer ones use more expansive data or algorithmic novelties.10 For our SUPERNI experiments in §4.3, we only compare with their text-davinci-001 engine, because their newer engines are trained with the latest user data and are likely to have already seen the SUPERNI test set. For our human evalua- tion on newly written instructions, we include their 001, 002 and 003 engines for completeness.

Additionally, to compare SELF-INSTRUCT train- ing with other publicly available instruction tuning data, we further finetune GPT3 model with data from PROMPTSOURCE and SUPERNI, which are used to train the T0 and T𝑘-INSTRUCT models. We call them T0 training and SUPERNI training for short, respectively. To save the training budget, we sampled 50K instances (but covering all their in- structions) for each dataset, which has a comparable size to the instruction data we generated. Based on the findings from Wang et al. (2022) and our early experiments, reducing the number of instances per training task does not degrade the model’s general- ization performance to unseen tasks.

### Experiment 1: Zero-Shot Generalization on SUPERNI benchmark

We first evaluate the models’ ability to follow in- structions on typical NLP tasks in a zero-shot fash- ion. We use the evaluation set of SUPERNI (Wang et al., 2022), which consists of 119 tasks with 100 in- stances in each task. In this work, we mainly focus on the zero-shot setup, i.e., the model is prompted with the definition of the tasks only, without in- context demonstration examples. For all our re- quests to the GPT3 variants, we use the determin- istic generation mode (temperature as 0 and no nu- cleus sampling) without specific stop sequences

Results. We make the following observations from the results in Table 3. SELF-INSTRUCT boosts the instruction-following ability of GPT3 by a large margin. The vanilla GPT3 model basically can- not follow human instructions at all. Upon manual analysis, we find that it usually generates irrelevant and repetitive text, and does not know when to stop generation. Compared with other mod- els that are not specifically trained for SUPERNI, GPT3SELF-INST achievesbetterperformancethanT0 or the GPT3 finetuned on the T0 training set, which takes tremendous human labeling efforts. Notably, GPT3SELF-INST alsonearlymatchestheperformance of InstructGPT001, which is trained with private user data and human-annotated labels.

Models trained on the SUPERNI training set still achieve better performance on its evaluation set, which we attribute to the similar instruction style and formatting. However, we show that SELF- INSTRUCT still brings in additional gains when com- bined with the SUPERNI training set, proving its value as complementary data.

### Experiment 2: Generalization to User-oriented Instructions on Novel Tasks

Despite the comprehensiveness of SUPERNI in col- lecting existing NLP tasks, most of these NLP tasks were proposed for research purposes and skewed toward classification. To better access the practi- cal value of instruction-following models, a sub- set of the authors curate a new set of instructions motivated by user-oriented applications. We first brainstorm various domains where large LMs may be useful (e.g., email writing, social media, pro- ductivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance (again, input is optional). We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, ta- bles, codes, equations, etc.). In total, we create 252 instructions with 1 instance per instruction. We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfa- miliar instructions. Table 9 presents a small portion of them. The entire set is available in our GitHub repository. We analyze the overlap between this set set and the seed instructions in §A.1.

**Human evaluation setup**


Evaluating models’ performance on this evaluation set of diverse tasks is extremely challenging because different tasks re-
quire different expertise. Indeed, many of these tasks cannot be measured by automatic metrics or even be judged by normal crowdworkers (e.g., writ- ing a program, or converting first-order logic into natural language). To get a more faithful evaluation, we asked the authors of the instructions to judge model predictions. Details on how we set up this human evaluation are described in Appendix B. The evaluators were asked to rate the output based on whether it accurately and effectively completes the task. We implemented a four-level rating system for categorizing the quality of the models’ outputs:

* RATING-A: The response is valid and satisfying.
* RATING-B: The response is acceptable but has minor errors or imperfections.
* RATING-C: The response is relevant and responds to the instruction, but it has significant errors in the content. For example, GPT3 might generate a valid output first, but continue to generate other irrelevant things.
* RATING-D: The response is irrelevant or completely invalid

**Results**. 

Figure 6 shows the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set (w. inter-rater agreement 𝜅 = 0.57 on the 4-class categorical scale, see Appendix B for details). As anticipated, the vanilla GPT3 LM is largely unable to respond to in- structions, and all instruction-tuned models demon- strate comparatively higher performance. Nonethe- less, GPT3SELF-INST (i.e., GPT3 model finetuned with SELF-INSTRUCT) outperforms those counter- parts trained on T0 or SUPERNI data by a large mar- gin, demonstrating the value of the generated data despite the noise. Compared with InstructGPT001, GPT3SELF-INST is quite close in performance—if we count acceptable response with minor imper- fections (RATING-B) as valid, GPT3SELF-INST is only 5% behind InstructGPT001. Lastly, our evalua- tion confirms the impressive instruction-following ability of InstructGPT002 and InstructGPT003. Al- though there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used by Ouyang et al. (2022).

### Effect of Data Size and Quality

Data size. SELF-INSTRUCT provides a way to grow instruction data at a low cost with almost no human labeling; could more of this generated data lead to better instruction-following ability? We conduct an analysis of the size of generated data by subsampling different numbers of instructions from the generated dataset, finetuning GPT3 on the sampled subsets, and evaluating how the resulting models perform on the 252 user-oriented instruc- tion set. We conduct the same human evaluation as in §4.4. Figure 7 presents the performance of GPT3SELF-INST modelsfinetunedwithdifferentsizes of generated data. Overall, we see consistent im- provement as we grow the data size. However, this improvement almost plateaus after 16K. This is in- line with the data scaling experiments in Wang et al. (2022, Fig. 5). Interestingly, when evaluating on SUPERNI we found the model’s performance gain plateaus earlier at around hundreds of instructions. This may be due to the fact that the new generated data is distinct from typical NLP tasks in SUPERNI, indicating that future research may benefit from us- ing a combination of different instruction data for better performance on various types of tasks.

**Data quality**. 

Another direction to improve the model’s performance is to take our generated data and get better supervision (with less noise). We explore this idea by using InstructGPT003 (the best available general-purpose model) to regenerate the output field of all our instances given the instruction and input. We then use this improved version of our data to finetune GPT3. This can be regarded as a distillation of InstructGPT003 with our data. As is shown in Figure 7, the resulting model outperforms the counterpart trained with the original data by 10%, which suggests big room for future work on using our generation pipeline to get initial data and then improving the data quality with human experts or distillation from better models.

## Related Work
Instruction-following LMs. A series of works have found evidence that vanilla LMs can be effec- tive at following general language instructions if tuned with annotated “instructional” data—datasets containing language instructional commands and their desired outcomes based on human annota- tion (Weller et al., 2020; Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022, i.a.). Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the general- izability of resulting models to unseen tasks (Wang et al., 2022; Chung et al., 2022). However, since
these developments largely focus on existing NLP tasks and depend on human-annotated instructions, this poses a bottleneck for progress toward more generalizable models (e.g., see Fig. 5a in Wang et al., 2022). Our work aims to move beyond classi- cal NLP tasks and tackle the challenges of creating diverse instruction data by employing pretrained LMs. InstructGPT (Ouyang et al., 2022) shares a similar goal as ours in building more general- purpose LMs, and has demonstrated remarkable performance in following diverse user instructions. However, as a commercial system, their construc- tion process still remains quite opaque. In partic- ular, the role of data has remained understudied due to limited transparency and the private user data they used in their study. Addressing such chal- lenges necessitates the creation of a large-scale, public dataset covering a broad range of tasks.
Language models for data generation and aug- mentation. A variety of works have proposed using LMs for data generation (Schick and Schütze, 2021; Wang et al., 2021; Liu et al., 2022; Meng et al., 2023) or augmentation (Feng et al., 2021; Yang et al., 2020; Mekala et al., 2022). Our work differs from this line in that it is not specific to a particular task (say, QA or NLI). In contrast, a dis- tinct motivation for SELF-INSTRUCT is to bootstrap new task definitions that may not have been defined

before by NLP practitioners (though potentially still important for real users). In parallel with our work, Honovich et al. (2022a) also propose to generate large-scale instruction data (so-called Unnatural Instructions) with GPT3 models. The major differ- ences are that 1) they use tasks in SUPERNI (Wang et al., 2022) as their seed tasks, resulting in a differ- ent distribution of generated tasks; 2) they employ InstructGPT002 for generating the data, in which sense they are distilling knowledge from an already instruction-tuned model, while we solely rely on the vanilla LM; 3) the detailed generation pipeline and templates are different. Nevertheless, we be- lieve that both efforts in expanding instruction data are complementary, and the community will benefit from these diverse datasets.
Instruction generation. A series of recent works (Zhou et al., 2022b; Ye et al., 2022; Singh et al., 2022; Honovich et al., 2022b) generate in- structions of a task given a few examples. While SELF-INSTRUCT also involves instruction genera- tion, a major difference in our case is it is task- agnostic; we generate new tasks (instructions along with instances) from scratch.
Model self-training. A typical self-training framework (He et al., 2019; Xie et al., 2020; Du et al., 2021; Amini et al., 2022; Huang et al., 2022) uses trained models to assign labels to unlabeled data and then leverages the newly labeled data to improve the model. In a similar line, Zhou et al. (2022a) use multiple prompts to specify a single task and propose to regularize via prompt consis- tency, encouraging consistent predictions over the prompts. This allows either finetuning the model with extra unlabeled training data, or direct applica- tion at inference time. While SELF-INSTRUCT has similarities with the self-training literature, most self-training methods assume a specific target task as well as unlabeled examples under it; in contrast, SELF-INSTRUCT produces a variety of tasks from scratch.
Knowledge distillation. Knowledge distilla- tion (Hinton et al., 2015; Sanh et al., 2019; West et al., 2021; Magister et al., 2022) often involves the transfer of knowledge from larger models to smaller ones. SELF-INSTRUCT can also be viewed as a form of “knowledge distillation", however, it differs from this line in the following ways: (1) the source and target of distillation are the same, i.e., a model’s knowledge is distilled to itself; (2)
the content of distillation is in the form of an instruction task (i.e., instructions that define a task, and a set of examples that instantiate it).
Bootstrapping with limited resources. A se- ries of recent works use language models to boot- strap some inferences using specialized methods. NPPrompt (Zhao et al., 2022) provides a method to generate predictions for semantic labels without any finetuning. It uses a model’s own embeddings to automatically find words relevant to the label of the data sample and hence reduces the dependency on manual mapping from model prediction to la- bel (verbalizers). STAR (Zelikman et al., 2022) iteratively leverages a small number of rationale examples and a large dataset without rationales, to bootstrap a model’s ability to perform reasoning. Self-Correction (Welleck et al., 2023) decouples an imperfect base generator (model) from a separate corrector that learns to iteratively correct imperfect generations and demonstrates improvement over the base generator. Our work instead focuses on boot- strapping new tasks in the instruction paradigm.
Multi-modal instruction-following. Instruction- following models have also been of interest in the multi-modal learning literature (Fried et al., 2018; Shridhar et al., 2020; Min et al., 2022; Weir et al., 2022). SELF-INSTRUCT, as a general approach to expanding data, can potentially also be helpful in those settings, which we leave to future work.

## Conclusion

We introduce SELF-INSTRUCT, a method to improve the instruction-following ability of LMs via their own generation of instruction data. On ex- perimenting with vanilla GPT3, we automatically construct a large-scale dataset of 52K instructions for diverse tasks, and finetuning GPT3 on this data leads to a 33% absolute improvement on SUPERNI over the original GPT3. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with SELF-INSTRUCT outperforms using ex- isting public instruction datasets by a large margin and performs closely to InstructGPT001. We hope SELF-INSTRUCT can serve as the first step to align pretrained LMs to follow human instructions, and future work can build on top of this data to improve instruction-following models.

## Broader Impact

Beyond the immediate focus of this paper, we believe that SELF-INSTRUCT may help bring more transparency to what happens “behind the scenes” of widely-used instruction-tuned models like InstructGPT or ChatGPT. Unfortunately, such industrial models remain behind API walls as their datasets are not released, and hence there is lit- tle understanding of their construction and why they demonstrate impressive capabilities. The bur- den now falls on academia to better understand the source of success in these models and strive for better—and more open—models. We believe our findings in this paper demonstrate the importance of diverse instruction data, and our large synthetic dataset can be the first step toward higher-quality data for building better instruction-following mod- els. At this writing, the central idea of this paper has been adopted in several follow-up works for such endeavors (Taori et al., 2023; Xu et al., 2023; Sun et al., 2023, i.a.).

## Limitations

Here, we discuss some limitations of this work to inspire future research in this direction.

Tail phenomena. SELF-INSTRUCT depends on LMs, and it will inherit all the limitations that carry over with LMs. As recent studies have shown (Razeghi et al., 2022; Kandpal et al., 2022), tail phenomena pose a serious challenge to the suc- cess of LMs. In other words, LMs’ largest gains correspond to the frequent uses of languages (head of the language use distribution), and there might be minimal gains in the low-frequency contexts. Similarly, in the context of this work, it would not be surprising if the majority of the gains by SELF- INSTRUCT are skewed toward tasks or instructions that present more frequently in the pretraining cor- pus. As a consequence, the approach might show brittleness with respect to uncommon and creative instructions.

Dependence on large models. Because of SELF- INSTRUCT’s dependence on the inductive biases extracted from LMs, it might work best for larger models. If true, this may create barriers to access for those who may not have large computing resources. We hope future studies will carefully study the gains as a function of model size or various other parame- ters. It is worthwhile to note that instruction-tuning with human annotation also suffers from a similar limitation: gains of instruction-tuning are higher for larger models (Wei et al., 2022).

Reinforcing LM biases. A point of concern for the authors is the unintended consequences of this iterative algorithm, such as the amplification of problematic social biases (stereotypes or slurs about gender, race, etc.). Relatedly, one observed chal- lenge in this process is the algorithm’s difficulty in producing balanced labels, which reflected models’ prior biases. We hope future work will lead to better understanding of the pros and cons of the approach.

## Acknowledgements

The authors would like to thank the anonymous reviewers for their constructive feedback. We espe- cially thank Sewon Min, Eric Wallace, Ofir Press, and other members of UWNLP and AllenNLP for their encouraging feedback and intellectual sup- port. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19- 2-4031), ONR N00014-18-1-2826, ONR MURI N00014-18-1-2670, and gifts from AI2 and an Allen Investigator award.